{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Lista de Exerc\u00edcios #12: Redes Neurais Convolucionais (CNN)\n",
                "\n",
                "**Aluno:** Samuel Horta de Faria\n",
                "\n",
                "**Matr\u00edcula:** 801528\n",
                "\n",
                "**Curso:** Ci\u00eancia da Computa\u00e7\u00e3o\n",
                "**Disciplina:** Intelig\u00eancia Artificial\n",
                "\n",
                "**Prof\u00aa:** Cristiane Neri Nobre\n",
                "\n",
                "Este notebook implementa uma Rede Neural Convolucional (CNN) para resolver o problema de classifica\u00e7\u00e3o de imagens da competi\u00e7\u00e3o 'Dogs vs. Cats' do Kaggle. O objetivo \u00e9 construir um modelo capaz de diferenciar imagens de cachorros e gatos. As etapas incluem a prepara\u00e7\u00e3o dos dados, a constru\u00e7\u00e3o da arquitetura da CNN, o treinamento do modelo e a gera\u00e7\u00e3o de um arquivo de submiss\u00e3o."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 0. Configura\u00e7\u00f5es Iniciais e Imports\n",
                "\n",
                "Nesta se\u00e7\u00e3o, importamos todas as bibliotecas necess\u00e1rias para o projeto. Utilizaremos `pandas` para manipula\u00e7\u00e3o de dados, `numpy` para opera\u00e7\u00f5es num\u00e9ricas, `matplotlib` para visualiza\u00e7\u00e3o, e `tensorflow.keras` para construir e treinar nossa CNN."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports para manipula\u00e7\u00e3o de dados e sistema operacional\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import random\n",
                "\n",
                "# Imports para visualiza\u00e7\u00e3o de dados\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Imports do TensorFlow e Keras para a constru\u00e7\u00e3o da CNN\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "\n",
                "# Import para divis\u00e3o de dados\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Configura\u00e7\u00e3o para exibir gr\u00e1ficos no notebook\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Defini\u00e7\u00e3o de Constantes e Caminhos\n",
                "\n",
                "Definir constantes no in\u00edcio do c\u00f3digo \u00e9 uma boa pr\u00e1tica, pois facilita a manuten\u00e7\u00e3o e a altera\u00e7\u00e3o de par\u00e2metros importantes, como o tamanho das imagens e os caminhos para os diret\u00f3rios de dados."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- ATEN\u00c7\u00c3O: Altere os caminhos para os diret\u00f3rios do seu computador ---\n",
                "TRAIN_DIR = 'dogs-vs-cats/train/'\n",
                "TEST_DIR = 'dogs-vs-cats/test1/'\n",
                "\n",
                "# Constantes para o modelo\n",
                "IMAGE_WIDTH = 128\n",
                "IMAGE_HEIGHT = 128\n",
                "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
                "IMAGE_CHANNELS = 3 # 3 para imagens coloridas (RGB)\n",
                "BATCH_SIZE = 32 # N\u00famero de imagens processadas por lote"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Prepara\u00e7\u00e3o dos Dados de Treino\n",
                "\n",
                "Os dados de treino est\u00e3o em uma \u00fanica pasta, com os nomes dos arquivos indicando a classe (`cat.x.jpg` ou `dog.x.jpg`). Vamos ler todos os nomes de arquivos, extrair suas classes e organizar essas informa\u00e7\u00f5es em um DataFrame do pandas para facilitar a manipula\u00e7\u00e3o."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lista para armazenar nomes de arquivos e suas categorias\n",
                "filenames = os.listdir(TRAIN_DIR)\n",
                "categories = []\n",
                "\n",
                "# Loop para extrair a categoria do nome do arquivo\n",
                "for filename in filenames:\n",
                "    category = filename.split('.')[0]\n",
                "    if category == 'dog':\n",
                "        categories.append('dog') # Usando strings para facilitar a divis\u00e3o\n",
                "    else:\n",
                "        categories.append('cat')\n",
                "\n",
                "# Cria\u00e7\u00e3o do DataFrame\n",
                "train_df = pd.DataFrame({\n",
                "    'filename': filenames,\n",
                "    'category': categories\n",
                "})\n",
                "\n",
                "print(\"---| DataFrame de Treino Criado |---\")\n",
                "print(f\"Total de imagens de treino: {train_df.shape[0]}\")\n",
                "print(train_df.head())\n",
                "\n",
                "print(\"\\n---| Distribui\u00e7\u00e3o das Classes |---\")\n",
                "print(train_df['category'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Divis\u00e3o dos Dados e Gera\u00e7\u00e3o de Lotes (Batches)\n",
                "\n",
                "Para avaliar o modelo de forma justa, dividimos o conjunto de treino em dois: um para treinamento (90%) e outro para valida\u00e7\u00e3o (10%). A valida\u00e7\u00e3o nos ajuda a monitorar se o modelo est\u00e1 sofrendo de overfitting (decorando os dados de treino em vez de aprender a generalizar).\n",
                "\n",
                "Usaremos `ImageDataGenerator` do Keras para:\n",
                "1.  **Aumento de Dados (Data Augmentation):** Criar varia\u00e7\u00f5es das imagens de treino (rota\u00e7\u00f5es, zooms, etc.) para tornar o modelo mais robusto.\n",
                "2.  **Gera\u00e7\u00e3o de Lotes:** Carregar as imagens da mem\u00f3ria de forma eficiente em lotes, redimensionando-as e normalizando os valores dos pixels (dividindo por 255)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Divis\u00e3o do DataFrame em treino e valida\u00e7\u00e3o\n",
                "train_df, validate_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['category'])\n",
                "train_df = train_df.reset_index(drop=True)\n",
                "validate_df = validate_df.reset_index(drop=True)\n",
                "\n",
                "print(\"---| Tamanho dos Conjuntos Ap\u00f3s a Divis\u00e3o |---\")\n",
                "print(f\"Treino: {train_df.shape[0]} imagens\")\n",
                "print(f\"Valida\u00e7\u00e3o: {validate_df.shape[0]} imagens\")\n",
                "\n",
                "# Gerador para dados de treino com Data Augmentation\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255.,\n",
                "    rotation_range=15,\n",
                "    shear_range=0.1,\n",
                "    zoom_range=0.2,\n",
                "    horizontal_flip=True,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1\n",
                ")\n",
                "\n",
                "# Gerador para dados de valida\u00e7\u00e3o (apenas normaliza\u00e7\u00e3o)\n",
                "validation_datagen = ImageDataGenerator(rescale=1./255.)\n",
                "\n",
                "# Criando os geradores que ler\u00e3o as imagens a partir do DataFrame\n",
                "train_generator = train_datagen.flow_from_dataframe(\n",
                "    train_df, \n",
                "    TRAIN_DIR,\n",
                "    x_col='filename',\n",
                "    y_col='category',\n",
                "    target_size=IMAGE_SIZE,\n",
                "    class_mode='binary', # Classifica\u00e7\u00e3o bin\u00e1ria (gato/cachorro)\n",
                "    batch_size=BATCH_SIZE\n",
                ")\n",
                "\n",
                "validation_generator = validation_datagen.flow_from_dataframe(\n",
                "    validate_df,\n",
                "    TRAIN_DIR,\n",
                "    x_col='filename',\n",
                "    y_col='category',\n",
                "    target_size=IMAGE_SIZE,\n",
                "    class_mode='binary',\n",
                "    batch_size=BATCH_SIZE\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Constru\u00e7\u00e3o do Modelo CNN\n",
                "\n",
                "Agora, definimos a arquitetura da nossa CNN. O modelo ser\u00e1 sequencial e composto por:\n",
                "- **Camadas Convolucionais (`Conv2D`):** Respons\u00e1veis por extrair caracter\u00edsticas das imagens (bordas, texturas, formas).\n",
                "- **Camadas de Pooling (`MaxPooling2D`):** Reduzem a dimensionalidade dos mapas de caracter\u00edsticas, mantendo as informa\u00e7\u00f5es mais importantes.\n",
                "- **Camada Flatten:** Transforma a matriz 2D de caracter\u00edsticas em um vetor 1D para ser processado pela parte final da rede.\n",
                "- **Camadas Densas (`Dense`):** Camadas totalmente conectadas que realizam a classifica\u00e7\u00e3o com base nas caracter\u00edsticas extra\u00eddas.\n",
                "- **Camada de Dropout:** T\u00e9cnica de regulariza\u00e7\u00e3o para prevenir overfitting, \"desligando\" aleatoriamente alguns neur\u00f4nios durante o treino.\n",
                "- **Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o:** `relu` para as camadas ocultas e `sigmoid` para a camada de sa\u00edda, pois temos uma classifica\u00e7\u00e3o bin\u00e1ria."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Sequential()\n",
                "\n",
                "# Bloco 1\n",
                "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "# Bloco 2\n",
                "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "# Bloco 3\n",
                "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "# Camada de achatamento e camadas densas\n",
                "model.add(Flatten())\n",
                "model.add(Dense(512, activation='relu'))\n",
                "model.add(Dropout(0.5)) # Dropout para regulariza\u00e7\u00e3o\n",
                "model.add(Dense(1, activation='sigmoid')) # Sa\u00edda com sigmoid para classifica\u00e7\u00e3o bin\u00e1ria\n",
                "\n",
                "# Compila\u00e7\u00e3o do modelo\n",
                "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
                "\n",
                "print(\"---| Resumo da Arquitetura do Modelo |---\")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Treinamento do Modelo\n",
                "\n",
                "Com o modelo e os geradores de dados prontos, iniciamos o treinamento usando o m\u00e9todo `fit()`. O treinamento ocorrer\u00e1 por um n\u00famero definido de `epochs` (\u00e9pocas). Em cada \u00e9poca, o modelo ver\u00e1 todas as imagens do conjunto de treino.\n",
                "\n",
                "*Nota: O treinamento pode demorar dependendo do hardware (CPU vs GPU).* "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPOCHS = 15 # Um n\u00famero razo\u00e1vel para um bom come\u00e7o. Pode ser aumentado para melhores resultados.\n",
                "\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=validation_generator,\n",
                "    validation_steps=validate_df.shape[0] // BATCH_SIZE,\n",
                "    steps_per_epoch=train_df.shape[0] // BATCH_SIZE\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Avalia\u00e7\u00e3o do Modelo\n",
                "\n",
                "Ap\u00f3s o treinamento, \u00e9 crucial avaliar o desempenho. Plotamos a acur\u00e1cia e a perda (loss) do treino e da valida\u00e7\u00e3o ao longo das \u00e9pocas. Isso nos permite visualizar o aprendizado e identificar sinais de overfitting (quando a acur\u00e1cia de treino continua subindo, mas a de valida\u00e7\u00e3o estagna ou cai)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
                "\n",
                "# Gr\u00e1fico da Acur\u00e1cia\n",
                "ax1.plot(history.history['accuracy'], color='b', label=\"Acur\u00e1cia de Treino\")\n",
                "ax1.plot(history.history['val_accuracy'], color='r', label=\"Acur\u00e1cia de Valida\u00e7\u00e3o\")\n",
                "ax1.set_xticks(np.arange(0, EPOCHS, 1))\n",
                "ax1.set_ylabel('Acur\u00e1cia')\n",
                "ax1.set_title('Acur\u00e1cia por \u00c9poca')\n",
                "ax1.legend()\n",
                "\n",
                "# Gr\u00e1fico da Perda\n",
                "ax2.plot(history.history['loss'], color='b', label=\"Perda de Treino\")\n",
                "ax2.plot(history.history['val_loss'], color='r', label=\"Perda de Valida\u00e7\u00e3o\")\n",
                "ax2.set_xticks(np.arange(0, EPOCHS, 1))\n",
                "ax2.set_ylabel('Perda')\n",
                "ax2.set_xlabel('\u00c9poca')\n",
                "ax2.set_title('Perda por \u00c9poca')\n",
                "ax2.legend()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Prepara\u00e7\u00e3o dos Dados de Teste e Gera\u00e7\u00e3o da Submiss\u00e3o\n",
                "\n",
                "Finalmente, usamos o modelo treinado para prever as classes das imagens no conjunto de teste. O processo \u00e9 semelhante ao da valida\u00e7\u00e3o: criamos um DataFrame para os arquivos de teste e um gerador para carreg\u00e1-los e normaliz\u00e1-los. As previs\u00f5es do modelo (probabilidades entre 0 e 1) s\u00e3o ent\u00e3o convertidas para as classes finais (0 para 'gato', 1 para 'cachorro') e formatadas em um arquivo .csv para submiss\u00e3o no Kaggle."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preparando o DataFrame de teste\n",
                "test_filenames = os.listdir(TEST_DIR)\n",
                "test_df = pd.DataFrame({\n",
                "    'filename': test_filenames\n",
                "})\n",
                "nb_samples = test_df.shape[0]\n",
                "\n",
                "# Gerador para os dados de teste\n",
                "test_gen = ImageDataGenerator(rescale=1./255.)\n",
                "test_generator = test_gen.flow_from_dataframe(\n",
                "    test_df,\n",
                "    TEST_DIR,\n",
                "    x_col='filename',\n",
                "    y_col=None, # N\u00e3o h\u00e1 r\u00f3tulos para os dados de teste\n",
                "    class_mode=None,\n",
                "    target_size=IMAGE_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False # Importante manter a ordem para a submiss\u00e3o\n",
                ")\n",
                "\n",
                "# Realizando as predi\u00e7\u00f5es\n",
                "predict = model.predict(test_generator, steps=np.ceil(nb_samples/BATCH_SIZE))\n",
                "\n",
                "# Convertendo probabilidades em classes (0 ou 1)\n",
                "# A classe 'dog' \u00e9 1, 'cat' \u00e9 0. O sigmoid > 0.5 indica maior probabilidade de ser a classe 1.\n",
                "test_df['category'] = np.where(predict > 0.5, 1, 0)\n",
                "\n",
                "# Criando o DataFrame de submiss\u00e3o\n",
                "submission_df = test_df.copy()\n",
                "submission_df['id'] = submission_df['filename'].str.split('.').str[0]\n",
                "submission_df['label'] = submission_df['category']\n",
                "submission_df.drop(['filename', 'category'], axis=1, inplace=True)\n",
                "\n",
                "print(\"\\n---| DataFrame de Submiss\u00e3o para Kaggle |---\")\n",
                "print(submission_df.head())\n",
                "\n",
                "# Gerando o arquivo de submiss\u00e3o\n",
                "submission_df.to_csv('submission_cnn.csv', index=False)\n",
                "print(\"\\nArquivo 'submission_cnn.csv' gerado com sucesso!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Conclus\u00e3o\n",
                "\n",
                "Neste notebook, implementamos um pipeline completo para classifica\u00e7\u00e3o de imagens com uma Rede Neural Convolucional. Cobrimos desde o pr\u00e9-processamento dos dados, passando pela defini\u00e7\u00e3o e treinamento de uma arquitetura de CNN, at\u00e9 a avalia\u00e7\u00e3o do desempenho e a gera\u00e7\u00e3o de um arquivo de predi\u00e7\u00f5es para o conjunto de teste. Os resultados mostram a efic\u00e1cia das CNNs para tarefas de vis\u00e3o computacional, e o modelo pode ser ainda mais aprimorado com mais \u00e9pocas de treinamento, uma arquitetura mais complexa ou t\u00e9cnicas de ajuste fino (fine-tuning)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}